{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1456, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>year</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>week_start_date</th>\n",
       "      <th>ndvi_ne</th>\n",
       "      <th>ndvi_nw</th>\n",
       "      <th>ndvi_se</th>\n",
       "      <th>ndvi_sw</th>\n",
       "      <th>precipitation_amt_mm</th>\n",
       "      <th>reanalysis_air_temp_k</th>\n",
       "      <th>...</th>\n",
       "      <th>reanalysis_relative_humidity_percent</th>\n",
       "      <th>reanalysis_sat_precip_amt_mm</th>\n",
       "      <th>reanalysis_specific_humidity_g_per_kg</th>\n",
       "      <th>reanalysis_tdtr_k</th>\n",
       "      <th>station_avg_temp_c</th>\n",
       "      <th>station_diur_temp_rng_c</th>\n",
       "      <th>station_max_temp_c</th>\n",
       "      <th>station_min_temp_c</th>\n",
       "      <th>station_precip_mm</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.103725</td>\n",
       "      <td>0.198483</td>\n",
       "      <td>0.177617</td>\n",
       "      <td>12.42</td>\n",
       "      <td>297.572857</td>\n",
       "      <td>...</td>\n",
       "      <td>73.365714</td>\n",
       "      <td>12.42</td>\n",
       "      <td>14.012857</td>\n",
       "      <td>2.628571</td>\n",
       "      <td>25.442857</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>29.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>19</td>\n",
       "      <td>1990-05-07</td>\n",
       "      <td>0.169900</td>\n",
       "      <td>0.142175</td>\n",
       "      <td>0.162357</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>22.82</td>\n",
       "      <td>298.211429</td>\n",
       "      <td>...</td>\n",
       "      <td>77.368571</td>\n",
       "      <td>22.82</td>\n",
       "      <td>15.372857</td>\n",
       "      <td>2.371429</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>31.7</td>\n",
       "      <td>22.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>1990-05-14</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.172967</td>\n",
       "      <td>0.157200</td>\n",
       "      <td>0.170843</td>\n",
       "      <td>34.54</td>\n",
       "      <td>298.781429</td>\n",
       "      <td>...</td>\n",
       "      <td>82.052857</td>\n",
       "      <td>34.54</td>\n",
       "      <td>16.848571</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>26.714286</td>\n",
       "      <td>6.485714</td>\n",
       "      <td>32.2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>41.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>21</td>\n",
       "      <td>1990-05-21</td>\n",
       "      <td>0.128633</td>\n",
       "      <td>0.245067</td>\n",
       "      <td>0.227557</td>\n",
       "      <td>0.235886</td>\n",
       "      <td>15.36</td>\n",
       "      <td>298.987143</td>\n",
       "      <td>...</td>\n",
       "      <td>80.337143</td>\n",
       "      <td>15.36</td>\n",
       "      <td>16.672857</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>27.471429</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>33.3</td>\n",
       "      <td>23.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>sj</td>\n",
       "      <td>1990</td>\n",
       "      <td>22</td>\n",
       "      <td>1990-05-28</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.262200</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.247340</td>\n",
       "      <td>7.52</td>\n",
       "      <td>299.518571</td>\n",
       "      <td>...</td>\n",
       "      <td>80.460000</td>\n",
       "      <td>7.52</td>\n",
       "      <td>17.210000</td>\n",
       "      <td>3.014286</td>\n",
       "      <td>28.942857</td>\n",
       "      <td>9.371429</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  city  year  weekofyear week_start_date   ndvi_ne   ndvi_nw   ndvi_se  \\\n",
       "0   sj  1990          18      1990-04-30  0.122600  0.103725  0.198483   \n",
       "1   sj  1990          19      1990-05-07  0.169900  0.142175  0.162357   \n",
       "2   sj  1990          20      1990-05-14  0.032250  0.172967  0.157200   \n",
       "3   sj  1990          21      1990-05-21  0.128633  0.245067  0.227557   \n",
       "4   sj  1990          22      1990-05-28  0.196200  0.262200  0.251200   \n",
       "\n",
       "    ndvi_sw  precipitation_amt_mm  reanalysis_air_temp_k  ...  \\\n",
       "0  0.177617                 12.42             297.572857  ...   \n",
       "1  0.155486                 22.82             298.211429  ...   \n",
       "2  0.170843                 34.54             298.781429  ...   \n",
       "3  0.235886                 15.36             298.987143  ...   \n",
       "4  0.247340                  7.52             299.518571  ...   \n",
       "\n",
       "   reanalysis_relative_humidity_percent  reanalysis_sat_precip_amt_mm  \\\n",
       "0                             73.365714                         12.42   \n",
       "1                             77.368571                         22.82   \n",
       "2                             82.052857                         34.54   \n",
       "3                             80.337143                         15.36   \n",
       "4                             80.460000                          7.52   \n",
       "\n",
       "   reanalysis_specific_humidity_g_per_kg  reanalysis_tdtr_k  \\\n",
       "0                              14.012857           2.628571   \n",
       "1                              15.372857           2.371429   \n",
       "2                              16.848571           2.300000   \n",
       "3                              16.672857           2.428571   \n",
       "4                              17.210000           3.014286   \n",
       "\n",
       "   station_avg_temp_c  station_diur_temp_rng_c  station_max_temp_c  \\\n",
       "0           25.442857                 6.900000                29.4   \n",
       "1           26.714286                 6.371429                31.7   \n",
       "2           26.714286                 6.485714                32.2   \n",
       "3           27.471429                 6.771429                33.3   \n",
       "4           28.942857                 9.371429                35.0   \n",
       "\n",
       "   station_min_temp_c  station_precip_mm  total_cases  \n",
       "0                20.0               16.0            4  \n",
       "1                22.2                8.6            5  \n",
       "2                22.8               41.4            4  \n",
       "3                23.3                4.0            3  \n",
       "4                23.9                5.8            6  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"dengue_features_train.csv\")\n",
    "df_lables = pd.read_csv(\"dengue_labels_train.csv\")\n",
    "# merging data\n",
    "data = pd.merge(df_train, df_lables, left_on= ['city','year','weekofyear'], \n",
    "                right_on=['city','year','weekofyear'])\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# fill missing values\n",
    "data = (data.fillna(method='ffill'))\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column names\n",
    "data.columns = data.columns.str.replace(\"station\", \"stn\")\n",
    "data.columns = data.columns.str.replace(\"reanalysis\", \"re_an\")\n",
    "data.columns = data.columns.str.replace(\"humidity\",\"hd\")\n",
    "data.columns = data.columns.str.replace(\"precipitation\",\"prec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.city = data.city.astype(\"category\")\n",
    "data.year = data.year.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['city_iq', 'city_sj', 'year_1990', 'year_1991', 'year_1992',\n",
       "       'year_1993', 'year_1994', 'year_1995', 'year_1996', 'year_1997',\n",
       "       'year_1998', 'year_1999', 'year_2000', 'year_2001', 'year_2002',\n",
       "       'year_2003', 'year_2004', 'year_2005', 'year_2006', 'year_2007',\n",
       "       'year_2008', 'year_2009', 'year_2010'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(data[['city','year']])\n",
    "dummies.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural networks it is important that all features be on the same scale. So I have standardaised numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.drop(columns = ['city','year','week_start_date','total_cases'])\n",
    "df_test = data[['total_cases']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df_train_std = scaler.fit_transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate((df_train_std, dummies), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1164, 1), (1164, 44))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train, df_test, test_size=0.2)\n",
    "train_y.shape, train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Multi-Layer Perceptron Regressor model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikagillela/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(6, 9), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(6,9), activation='relu',max_iter=1000)\n",
    "mlp_reg.fit(train_x, train_y.values.ravel()) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.19278624469015\n"
     ]
    }
   ],
   "source": [
    "pred_reg =  mlp_reg.predict(test_x)\n",
    "print(mlp_reg.loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 339.205232560615\n",
      "Mean Aboslute Error : 10.223614931595362\n",
      "Root Mean Squared Error  18.417525147549412\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print('Mean Squared Error :',mean_squared_error(test_y, pred_reg))\n",
    "print('Mean Aboslute Error :',mean_absolute_error(test_y, pred_reg))\n",
    "print('Root Mean Squared Error ',sqrt(mean_squared_error(test_y, pred_reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_features = pd.read_csv(\"dengue_features_test.csv\")\n",
    "dengue_features.columns = dengue_features.columns.str.replace(r\"reanalysis\", \"re_an\")\n",
    "dengue_features.columns = dengue_features.columns.str.replace(r\"humidity\", \"hd\")\n",
    "dengue_features.columns = dengue_features.columns.str.replace(r\"precipitation\",\"prec\")\n",
    "dengue_features = dengue_features.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dengue_features['city'] = dengue_features['city'].astype('category')\n",
    "dengue_features['year'] = dengue_features['year'].astype('category')\n",
    "df_dummies = pd.get_dummies(dengue_features[['city','year']])\n",
    "df_dummies = dengue_features.reindex(columns = dummies.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_pred_data = scaler.transform(dengue_features.drop(columns =['city', 'year','week_start_date']))\n",
    "test_data = np.concatenate((st_pred_data, df_dummies), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harikagillela/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp_reg.predict(test_data)\n",
    "predictions = predictions.astype(\"int64\")\n",
    "submission = dengue_features[['city','year','weekofyear']]\n",
    "submission['total_cases'] = predictions\n",
    "submission.to_csv(\"submission_MLP.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare the models SGD, SVC and MLP \n",
    "<Br>\n",
    "The SGD Regressor has error of 15.6\n",
    "<br>\n",
    "The SVC model has error of 38.7\n",
    "<br>\n",
    "The MLP Regressor has an error of 23.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a new column called 'above_average' with value 1 or 0. 1 if the total_cases > median of total_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    737\n",
       "0    719\n",
       "Name: above_average, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['above_average'] = np.where(data['total_cases']>=12, '1', '0')\n",
    "data.above_average.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.drop(columns = ['city','year','week_start_date','total_cases','above_average'])\n",
    "df_test = data['above_average']\n",
    "df_train_std = scaler.fit_transform(df_train)\n",
    "train = np.concatenate((df_train_std, dummies), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network MLP Classifier on the 'above_average' column with 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(train, df_test, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Neural Network MLP Classifier on the 'above_average' column with 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000)\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_pred = mlp_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8732876712328768\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "print(accuracy_score(test_y, test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[129,  21],\n",
       "       [ 16, 126]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cnf_matrix = confusion_matrix(test_y, test_y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Precision__ The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "The precision is the ratio TP / (TP + FP) \n",
    "<br>\n",
    "__Recall__ The recall is  the ability of the classifier to find all the positive samples. The recall is the ratio TP / (TP + FN) \n",
    "<br>\n",
    "__F1-Score__ The F-mesaure is the harmonic mean of the precision and recall. F1 = 2 * (precision * recall) / (precision + recall)\n",
    "<br>\n",
    "We use these metrics for the evaluate a classification problem as these metrics ifdentify how precisely model can classify.accuracy is not a good metric for classification, model can be wrong though it has high accuracy. Accuracy favours the major class of the classification data. The metrics recall and precision favours the minory class of data which is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.873399\n",
      "Recall: 0.873662\n",
      "F1 score: 0.873274\n"
     ]
    }
   ],
   "source": [
    "# precision \n",
    "precision = precision_score(test_y, test_y_pred,average = \"macro\")\n",
    "print('Precision: %f' % precision)\n",
    "# recall\n",
    "recall = recall_score(test_y, test_y_pred, average = \"macro\")\n",
    "print('Recall: %f' % recall)\n",
    "# f1\n",
    "f1 = f1_score(test_y, test_y_pred, average = \"macro\")\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High precision relates the low false positive rate. Recall relates to the high true positive rate. The recall and precision for our model of suggest that model is good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
